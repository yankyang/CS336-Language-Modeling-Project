# CS336 Language Modeling Project â€” Tiny Transformer from Scratch

This project is my personal implementation inspired by **Stanford CS336 (Language Modeling from Scratch)**.  
It builds a minimal end-to-end language model training pipeline using only basic PyTorch modules:

- âœ”ï¸ Word-level tokenizer  
- âœ”ï¸ Tiny Transformer LM (attention + MLP + positional embeddings)  
- âœ”ï¸ Training loop (checkpoint saving included)  
- âœ”ï¸ Simple evaluation (perplexity)  
- âœ”ï¸ Sampling (text generation)  

## ğŸ“‚ Project Structure

CS336-Language-Modeling-Project/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ tokenizer.py # Word-level tokenizer
â”‚ â”œâ”€â”€ model.py # Tiny Transformer model
â”‚ â”œâ”€â”€ dataset.py # Next-token dataset
â”‚ â”œâ”€â”€ train.py # Training script
â”‚ â”œâ”€â”€ sampling.py # Text generation
â”‚ â””â”€â”€ eval_ppl.py # Perplexity evaluation
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ tiny_corpus.txt # Small training corpus
â”‚
â”œâ”€â”€ checkpoints/ # Saved model checkpoints
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE


---

## ğŸš€ Quick Start

### 1. Install environment

```bash
python3 -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt

### 2. Prepare data

Place your training text under:
data/tiny_corpus.txt
You can use any plain-English text.
ğŸ§  Train the Tiny Transformer
python -m src.train \
  --data_path data/tiny_corpus.txt \
  --block_size 64 \
  --batch_size 32 \
  --n_epochs 5


This will:

build a vocabulary â†’ data/vocab.json

train the LM

save checkpoints â†’ checkpoints/epoch_*.pt

âœ¨ Generate Text

After training, run:

python -m src.sampling \
  --checkpoint checkpoints/epoch_3.pt \
  --vocab_path data/vocab.json \
  --prompt "once upon a time" \
  --max_new_tokens 80 \
  --temperature 0.8 \
  --top_k 20


Example output:

once upon a time in a small village a traveler carried a book of stories...

ğŸ“ Evaluate Perplexity
python -m src.eval_ppl \
  --data_path data/tiny_corpus.txt \
  --checkpoint checkpoints/epoch_3.pt \
  --vocab_path data/vocab.json \
  --block_size 64 \
  --batch_size 32


Output:

Perplexity: 21.84

ğŸ““ Optional: Notebook Demo

The project supports Jupyter Notebook.
You can create a notebook under notebooks/ and reuse the modules:

from src.tokenizer import Tokenizer
from src.model import TinyTransformerLM
from src.sampling import sample

ğŸ”§ Model Architecture (Tiny)

Embedding + learned positional embeddings

Multi-head self-attention (causal mask)

Feed-forward MLP

LayerNorm + residual connections

Linear output projection to vocab logits

This small model is ideal for learning:

how attention works

how an LM predicts next tokens

how perplexity relates to LM quality

ğŸ“Œ Future Improvements (Optional Ideas)

Add validation split + early stopping

Implement BPE tokenizer

Add multi-layer attention visualizations

Train on larger corpora (TinyStories, WikiText)

Export to ONNX or TorchScript
